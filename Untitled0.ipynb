{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/owlweed/S.E.S/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In9FyaSGI5-A",
        "outputId": "3be8c728-0348-4142-c85f-12b1c2901239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-c94b54563c27>:24: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:, (data.columns != 'Class') & (data.columns != 'text')] = data.loc[:, (data.columns != 'Class') & (data.columns != 'text')].applymap(lambda x: np.nan if (not isinstance(x, (int, float))) else x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 정확도: 0.9710867397806581\n",
            "새로운 PDF의 악성 여부 예측:\n",
            "             Feature  Count\n",
            "0            pdfsize      5\n",
            "1      metadata size     10\n",
            "2              pages      3\n",
            "3        xref Length      1\n",
            "4   title characters      1\n",
            "5        isEncrypted      1\n",
            "6     embedded files      1\n",
            "7             images      1\n",
            "8               text      1\n",
            "9                obj      1\n",
            "10            endobj      1\n",
            "11            stream      1\n",
            "12         endstream      1\n",
            "13              xref      1\n",
            "14           trailer      1\n",
            "15         startxref      1\n",
            "16            pageno      1\n",
            "17           encrypt      1\n",
            "18            ObjStm      1\n",
            "19                JS      1\n",
            "20        Javascript      1\n",
            "21                AA      1\n",
            "22        OpenAction      1\n",
            "23          Acroform      1\n",
            "24       JBIG2Decode      1\n",
            "25         RichMedia      1\n",
            "26            launch      1\n",
            "27      EmbeddedFile      1\n",
            "28               XFA      1\n",
            "29            Colors      1\n",
            "30             Class      1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hist_gradient_boosting_model(2024).pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer  # Import the SimpleImputer class\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# CSV 파일에서 데이터 로드\n",
        "csv_file_path = \"/content/PDFMalware2024.csv\"  # CSV 파일 경로를 지정하세요.\n",
        "data = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Class 열을 레이블로 변환 (Malicious를 1, Benign을 0으로 매핑)\n",
        "data[\"Class\"] = data[\"Class\"].map({\"Malicious\": 1, \"Benign\": 0})\n",
        "\n",
        "# 'text' 열의 값을 변경\n",
        "data['text'] = data['text'].map({'No': 0, 'Yes': 1, 'unclear': 2})\n",
        "\n",
        "\n",
        "# 모든 열(class 제외)에 대해 숫자가 아닌 값을 NaN으로 설정\n",
        "data.loc[:, (data.columns != 'Class') & (data.columns != 'text')] = data.loc[:, (data.columns != 'Class') & (data.columns != 'text')].applymap(lambda x: np.nan if (not isinstance(x, (int, float))) else x)\n",
        "\n",
        "# 나머지 NaN 값들을 최빈값(mode)로 대체\n",
        "data.fillna(data.mode().iloc[0], inplace=True)\n",
        "\n",
        "\n",
        "# 선택할 열 이름 목록\n",
        "selected_columns = ['obj', 'endobj', 'stream', 'endstream', 'xref', 'trailer', 'startxref', 'pageno']\n",
        "\n",
        "# \"Fine namer\"와 \"header\" 열을 제외한 특성과 레이블 분리\n",
        "X = data.drop(columns=[\"Class\"])  # 레이블 및 제외할 열을 제외한 모든 열을 특성으로 사용\n",
        "y = data[\"Class\"]  # 레이블 열은 악성(1) 또는 양성(0)을 나타내는 열로 가정\n",
        "\n",
        "# 학습 및 테스트 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# HistGradientBoostingClassifier 모델 생성 및 학습\n",
        "model = HistGradientBoostingClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 모델 예측\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 정확도 평가\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"모델 정확도: {accuracy}\")\n",
        "\n",
        "# 새로운 PDF 특성 데이터로 악성 여부 예측\n",
        "new_data = pd.DataFrame({\"Feature\": [\"pdfsize\", \"metadata size\", \"pages\", \"xref Length\", \"title characters\", \"isEncrypted\", \"embedded files\", \"images\", \"text\", \"obj\", \"endobj\", \"stream\", \"endstream\", \"xref\", \"trailer\", \"startxref\", \"pageno\", \"encrypt\", \"ObjStm\", \"JS\", \"Javascript\", \"AA\", \"OpenAction\", \"Acroform\", \"JBIG2Decode\", \"RichMedia\", \"launch\", \"EmbeddedFile\", \"XFA\", \"Colors\", \"Class\"], \"Count\": [5, 10, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})  # 새로운 데이터 예시\n",
        "\n",
        "print(\"새로운 PDF의 악성 여부 예측:\")\n",
        "print(new_data)\n",
        "\n",
        "# 모델을 파일로 저장\n",
        "model_filename = \"hist_gradient_boosting_model(2024).pkl\"\n",
        "joblib.dump(model, model_filename)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zjYkQpVB_tU",
        "outputId": "4f11f012-e03c-4a78-979a-09daa1ade051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install PyPDF2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY5kAqht5D7C",
        "outputId": "86999d73-a028-44b9-91f8-cf59af899a2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.23.7-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.7 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.23.7 pymupdf-1.23.7\n"
          ]
        }
      ],
      "source": [
        "pip install pymupdf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "from PyPDF2 import PdfReader\n",
        "import statistics\n",
        "import re\n",
        "import json\n",
        "import base64\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_pdf_features(pdf_file_path):\n",
        "    with open(pdf_file_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PdfReader(pdf_file)\n",
        "\n",
        "        # PDF 파일의 페이지 수\n",
        "        page_count = len(pdf_reader.pages)\n",
        "\n",
        "         # PDF 정보 가져오기\n",
        "        pdf_info = pdf_reader.metadata\n",
        "\n",
        "        # PDF 파일 크기\n",
        "        pdf_size = os.path.getsize(pdf_file_path)\n",
        "\n",
        "        # PDF 타이틀\n",
        "        title = pdf_info.title if 'title' in pdf_info else ''\n",
        "\n",
        "        # 암호화 여부\n",
        "        is_encrypted = pdf_reader.is_encrypted\n",
        "\n",
        "        # Header 확인 (첫 페이지의 텍스트)\n",
        "        first_page = pdf_reader.pages[0]\n",
        "        header_text = first_page.extract_text()  # 첫 페이지에서 텍스트 추출\n",
        "        has_header = bool(header_text.strip())  # 텍스트가 비어있지 않으면 Header가 있는 것으로 간주\n",
        "\n",
        "        image_count = 0\n",
        "        for page_num in range(page_count):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            if '/XObject' in page['/Resources']:\n",
        "                xobjects = page['/Resources']['/XObject']\n",
        "                if isinstance(xobjects, dict):\n",
        "                    # If xobjects is a dictionary, it means it contains references to external objects\n",
        "                    for xobj in xobjects.values():\n",
        "                        xobj = xobj.get_object()\n",
        "                        if '/Subtype' in xobj and xobj['/Subtype'] == '/Image':\n",
        "                            image_count += 1\n",
        "                elif isinstance(xobjects, PyPDF2.generic.ArrayObject):\n",
        "                    # If xobjects is an array, it means it directly contains XObject objects\n",
        "                    for xobj in xobjects:\n",
        "                        xobj = xobj.get_object()\n",
        "                        if '/Subtype' in xobj and xobj['/Subtype'] == '/Image':\n",
        "                            image_count += 1\n",
        "\n",
        "    with fitz.open(pdf_file_path) as pdf_doc:\n",
        "    # 텍스트 추출 (페이지별로 텍스트 유무에 따라 Yes 또는 No로 설정)\n",
        "        all_text = \"\"\n",
        "        for page_num in range(pdf_doc.page_count):\n",
        "            page = pdf_doc[page_num]\n",
        "            page_text = page.get_text()\n",
        "\n",
        "        # 페이지에 텍스트가 있는지 여부에 따라 Yes 또는 No로 설정\n",
        "        text_exists = \"Yes\" if page_text.strip() else \"No\"\n",
        "        all_text += text_exists\n",
        "\n",
        "\n",
        "        # Object 수 (PDF 내의 모든 객체 수)\n",
        "        object_count = len(pdf_reader.pages)\n",
        "\n",
        "        # Font Objects 수 (폰트 관련 객체 수)\n",
        "        font_objects = 0\n",
        "    for page_num in range(page_count):\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        if '/Font' in page['/Resources']:\n",
        "            fonts = page['/Resources']['/Font'].get_object()\n",
        "            font_objects += len(fonts)\n",
        "\n",
        "        # Embedded Files 수 (PDF에 포함된 파일 수)\n",
        "        embedded_files = len(pdf_reader.named_destinations)\n",
        "\n",
        "        # 검색할 키워드 목록\n",
        "        keywords = [\n",
        "            \"stream\",\n",
        "            \"endstream\",\n",
        "            \"JS\",\n",
        "            \"Javascript\",\n",
        "            \"/URI\",\n",
        "            \"/Action\",\n",
        "            \"AA\",\n",
        "            \"OpenAction\",\n",
        "            \"launch\",\n",
        "            \"/submitForm\",\n",
        "            \"Acroform\",\n",
        "            \"XFA\",\n",
        "            \"JBIG2Decode\",\n",
        "            \"Colors\",\n",
        "            \"RichMedia\",\n",
        "            \"trailer\",\n",
        "            \"xref\",\n",
        "            \"startxref\"\n",
        "        ]\n",
        "\n",
        "        # Count occurrences of each keyword\n",
        "        keyword_occurrences = {}\n",
        "        for keyword in keywords:\n",
        "            keyword_occurrences[keyword] = all_text.count(keyword)\n",
        "\n",
        "        # 특성 딕셔너리 생성\n",
        "        features = {\n",
        "            \"pdfsize\": pdf_size,\n",
        "            \"title characters\": len(title),\n",
        "            \"isEncrypted\": is_encrypted,\n",
        "            \"pages\": page_count,\n",
        "            \"header\": has_header,\n",
        "            \"images\": image_count,\n",
        "            \"text\": all_text,\n",
        "            \"obj\": object_count,\n",
        "            \"Font Objects\": font_objects,\n",
        "            \"embedded files\": embedded_files,\n",
        "            **keyword_occurrences  # Add keyword occurrences to features\n",
        "        }\n",
        "\n",
        "        return features\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 파일 경로를 받아옴\n",
        "    pdf_file_path = input(\"Enter the path of the PDF file: \")\n",
        "\n",
        "    # 특성 추출\n",
        "    features = extract_pdf_features(pdf_file_path)\n",
        "\n",
        "    # 결과를 CSV 파일로 저장\n",
        "    output_csv_path = \"output_features.csv\"\n",
        "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = features.keys()\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        # CSV 파일의 헤더 작성\n",
        "        writer.writeheader()\n",
        "\n",
        "        # 특성 값 작성\n",
        "        writer.writerow(features)\n",
        "\n",
        "    print(f\"Features extracted and saved to {output_csv_path}\")\n"
      ],
      "metadata": {
        "id": "1OQgkdjavL6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbc33918-e4c4-4636-e177-5cc537f9e41d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path of the PDF file: /content/Blue and White Project Proposal - Presentation.pdf\n",
            "Features extracted and saved to output_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# CSV 파일 경로\n",
        "csv_file_path = \"/content/output_features.csv\"\n",
        "\n",
        "# CSV 파일에서 데이터 로드\n",
        "data = pd.read_csv(csv_file_path)\n",
        "\n",
        "# 모델 로드\n",
        "model_path = \"hist_gradient_boosting_model.pkl\"\n",
        "model = joblib.load(model_path)\n",
        "\n",
        "# 'text' 열의 값을 변경\n",
        "data['text'] = data['text'].map({'No': 0, 'Yes': 1, 'unclear': 2})\n",
        "\n",
        "\n",
        "# 모든 열(class 제외)에 대해 숫자가 아닌 값을 NaN으로 설정\n",
        "data.loc[:, (data.columns != 'Class') & (data.columns != 'text')] = data.loc[:, (data.columns != 'Class') & (data.columns != 'text')].applymap(lambda x: np.nan if (not isinstance(x, (int, float))) else x)\n",
        "\n",
        "# 나머지 NaN 값들을 최빈값(mode)로 대체\n",
        "data.fillna(data.mode().iloc[0], inplace=True)\n",
        "\n",
        "\n",
        "# 선택할 열 이름 목록\n",
        "selected_columns = ['obj', 'endobj', 'stream', 'endstream', 'xref', 'trailer', 'startxref', 'pageno']\n",
        "\n",
        "\n",
        "# 특성 및 레이블 분리\n",
        "X_data = data.drop(columns=[\"header\"])\n",
        "# 예측\n",
        "predictions = model.predict(X_data)\n",
        "\n",
        "# 예측 결과를 데이터프레임에 추가\n",
        "data[\"Predicted_Class\"] = predictions\n",
        "\n",
        "# 결과 출력\n",
        "print(\"새로운 PDF의 악성 여부 예측:\")\n",
        "print(data[[\"Fine name\", \"header\", \"Predicted_Class\"]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "oObsN_P_Mq8R",
        "outputId": "05b8b08d-26fc-44fd-ca78-98e63426fcbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-fc19985594d5>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mX_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# 예측\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# 예측 결과를 데이터프레임에 추가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1872\u001b[0m         \"\"\"\n\u001b[1;32m   1873\u001b[0m         \u001b[0;31m# TODO: This could be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m         \u001b[0mencoded_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1875\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1910\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m         \"\"\"\n\u001b[0;32m-> 1912\u001b[0;31m         \u001b[0mraw_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1913\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\u001b[0m in \u001b[0;36m_raw_predict\u001b[0;34m(self, X, n_threads)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0mis_binned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_in_fit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_binned\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             X = self._validate_data(\n\u001b[0m\u001b[1;32m   1023\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_DTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mvalidated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 )\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     def _validate_data(\n",
            "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- /Action\n- /URI\n- /submitForm\n- Font Objects\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# 모델 로드\n",
        "model_filename = \"hist_gradient_boosting_model.pkl\"\n",
        "model = joblib.load(model_filename)\n",
        "\n",
        "# 주어진 새로운 데이터\n",
        "new_data = pd.DataFrame({\n",
        "    \"pdfsize\": [8],\n",
        "    \"pages\": [1],\n",
        "    \"title characters\": [0],\n",
        "    \"isEncrypted\": [0],\n",
        "    \"embedded files\": [0],\n",
        "    \"images\": [0],\n",
        "    \"text\": [0],\n",
        "    \"obj\": [10],\n",
        "    \"stream\": [3],\n",
        "    \"endstream\": [3],\n",
        "    \"xref\": [1],\n",
        "    \"trailer\": [1],\n",
        "    \"startxref\": [1],\n",
        "    \"JS\": [1],\n",
        "    \"Javascript\": [1],\n",
        "    \"AA\": [1],\n",
        "    \"OpenAction\": [1],\n",
        "    \"Acroform\": [1],\n",
        "    \"JBIG2Decode\": [1],\n",
        "    \"RichMedia\": [1],\n",
        "    \"launch\": [1],\n",
        "    \"XFA\": [1],\n",
        "    \"Colors\": [1],\n",
        "})\n",
        "\n",
        "# 예측\n",
        "predictions = model.predict(new_data)\n",
        "\n",
        "# 예측 결과 출력\n",
        "print(\"새로운 PDF의 악성 여부 예측:\")\n",
        "if predictions[0] == 1:\n",
        "    print(\"PDF는 악성입니다.\")\n",
        "else:\n",
        "    print(\"PDF는 악성이 아닙니다.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7aoIzTgZSjB",
        "outputId": "042f443e-99a1-4e04-e132-5781f71b94d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "새로운 PDF의 악성 여부 예측:\n",
            "PDF는 악성입니다.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEK0MgvpD5cPQC8l/T+BgT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}